{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read, Clean, and List Words to Use\n",
    "I need to get a vocab size to use in my computational graph, so I need to have a list of words to use to get the length. I'll use this to preprocess the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in(file):\n",
    "    with open(filepath, \"r\") as filetext:\n",
    "        word_ls = filetext.read().replace(\"\\n\", \"<eos>\").split()\n",
    "    # Clean the vocab from the list and return clean list\n",
    "    random = re.compile(r'[.a-zA-Z0-9]')\n",
    "    return [i for i in ls_words if (random.search(i) or i == '<eos>')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Batch Sizes\n",
    "\n",
    "An epoch is composed of one or more batches. Batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the model's internal parameters are updated, while epochs is also a hyperparameter of gradient descent that controls the number of complete passes through the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(data, batch_size, num_steps):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for i in range(len(data)):\n",
    "        if i > num_steps - 1:\n",
    "            x_data.append(data[i - num_steps:i])\n",
    "            y_data.append(data[i])\n",
    "    batches = int(len(x_data) / batch_size)\n",
    "    batch_out = list()\n",
    "    for i in range(batches):\n",
    "        # Per each batch\n",
    "        start_i = batch_size\n",
    "        end_i = start_i + batch_size\n",
    "        x_values = x_data[start_i:end_i]\n",
    "        y_values = y_data[start_i:end_i]\n",
    "        batch_out.append([x_values, v_values])\n",
    "    return batch_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Up the Brown Corpus Into Workable Bites\n",
    "For this, I'm actually going away from Bengio's implementation (might come back to bite me, but meh). Instead of 800,000 words in the training dataset, I'm only going to put in half of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_brown():\n",
    "    # retrieving the brown.txt file\n",
    "    with open('data/brown.txt') as file:\n",
    "        brown_list = file.read().split()\n",
    "        \n",
    "    # creating the training dataset\n",
    "    training_file = open(\"data/b_train.txt\", \"w\")\n",
    "    training_brown = ' '.join(brown_list[:400000])\n",
    "    training_file.write(training_brown)\n",
    "    training_file.close()\n",
    "    \n",
    "    # creating the validation dataset\n",
    "    validation_file = open(\"data/b_valid.txt\", \"w\")\n",
    "    validation_brown = ' '.join(brown_list[400000:500000])\n",
    "    validation_file.write(training_brown)\n",
    "    validation_file.close()\n",
    "    \n",
    "    # creating the testing dataset\n",
    "    testing_file = open(\"data/b_test.txt\", \"w\")\n",
    "    testing_brown = ' '.join(brown_list[500000:600000])\n",
    "    testing_file.write(testing_brown)\n",
    "    testing_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Configuration \n",
    "\n",
    "I found it easier to use `sys.argv[]` to create command lines than `argparse`, which I saw several others do. With sys.argv, it takes in all extra strings that come after the initial `python ____.py` as list items, and I can just assign variables to the item in the list.\n",
    "\n",
    "For the configurations, I'm going off of what Bengio has listed on page 1149 for his comparative results. For this assignment, we were instructed to compare our own results from MLP1, MLP3, MLP5, MLP7 and MLP9 for both the Brown and WikiText corpora. I just made each configuration a dictionary of dictionaries, with each key being the name of the configuration, followed by the second key being the name of the variable, with the value being what Bengio assigned them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-30648acafd46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcorpora\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mconfigurations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpotential_corpora\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbrown\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "mode = sys.argv[1]\n",
    "corpus = sys.argv[2]\n",
    "config = sys.argv[3]\n",
    "\n",
    "potential_modes = ['train', 'restore']\n",
    "\n",
    "potential_corpora = ['brown', 'wiki'] \n",
    "\n",
    "potential_configurations = {\n",
    "    'MLP1': {'num_steps':5, 'hidden_units':50, 'word_features':60, 'direct': True},\n",
    "    'MLP3': {'num_steps':5, 'hidden_units':0, 'word_features':60, 'direct': True},\n",
    "    'MLP5': {'num_steps':5, 'hidden_units':50, 'word_features':30, 'direct': True},\n",
    "    'MLP7': {'num_steps':3, 'hidden_units':50, 'word_features':30, 'direct': True},\n",
    "    'MLP9': {'num_steps':5, 'hidden_units':100, 'word_features':30, 'direct': False}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Parameters of the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    self.batch_size = 128\n",
    "    self.word_features = run_configuration['word_features']\n",
    "    self.num_steps = run_configuration['num_steps']\n",
    "    self.hidden_units = run_configuration['hidden_units']\n",
    "    self.direct_connections = run_configuration['direct']\n",
    "    \n",
    "    # to use a GPU or CPU \n",
    "    if tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None):\n",
    "        self.device = '/gpu:0'\n",
    "        print('Currently using GPU device to run code')\n",
    "    # need to figure out how to run this on a TPU\n",
    "    else:\n",
    "        self.device = '/cpu:0'\n",
    "        print('There is no GPU available, using CPU device to run code')\n",
    "    \n",
    "    # for the stochastic gradient descent\n",
    "    self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "    slr = 0.001\n",
    "    self.learning_rate = tf.train.exponential_decay(slr, self.global_step, 100, 0.96, staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def placeholders():\n",
    "    self.training_inputs = tf.placeholder(tf.float32, shape=[self.batch_size, self.num_steps])\n",
    "    self.training_labels = tf.placeholder(tf.float32, shape=[batch_size, [None]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(self):\n",
    "    with tf.device(self.device):\n",
    "        self.embedding_matrix = tf.Variable(tf.random_uniform([V, self.word_features], -1.0, 1.0))\n",
    "        # might need to flatten training_inputs \n",
    "        self.embedding_inputs = tf.nn.embed_lookup(embedding_matrix, training_inputs)\n",
    "        self.xt = tf.reshape(embed_inputs, (self.batch_size, self.z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self):\n",
    "    self.z = self.word_features * self.num_steps\n",
    "    with tf.device(self.device):\n",
    "        # output biases\n",
    "        b = tf.Variable(tf.random_uniform([V]))\n",
    "        # hidden layer biases\n",
    "        d = tf.Variable(tf.random_uniform([self.hidden_units]))\n",
    "\n",
    "        # W (word features to output weights)\n",
    "        if direct_connections == True:\n",
    "            W = tf.Variable(tf.random_normal([z, V], -1.0, 1.0))\n",
    "        else:\n",
    "            W = tf.Variable(np.zeros([z, V]), trainable=False)\n",
    "\n",
    "        # H (hidden layer weights)\n",
    "        H = tf.Variable(tf.random_uniform([z, self.hidden_units], -1.0, 1.0))\n",
    "\n",
    "        # U (hidden-to-output weights)\n",
    "        U = tf.Variable(tf.random_uniform([self.hidden_units, V]))\n",
    "\n",
    "        # building the graph (i.e. just a set of matrix multiplications)\n",
    "        hidden = tf.tanh(tf.matmul(self.xt, H) + d)\n",
    "        hidden2out = tf.matmul(hidden, U) + b\n",
    "        self.logits = tf.matmul(self.xt, W) + hidden2out\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=tf.one_hot(self.training_labels, V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the optimizer\n",
    "def optimizer(self):\n",
    "    with tf.device(self.device):\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training w/ `tf.Session`\n",
    "\n",
    "I can use the `feed_dict` object to feed values to the placeholders that I defined earlier in the graph. This will allow TF to compute the `loss` variable, but I'll also need to add an optimizer (very easy w/ TensorFlow). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(self):\n",
    "    # For tensorboard visualization, run main.py then in command line prompt type:\n",
    "    # tensorboard --logdir=\"./graphs\" --port 6006\n",
    "    # then open browser to http://localhost:6006/\n",
    "    tf.summary.scalar(\"loss\", self.loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(self):\n",
    "    self.placeholders()\n",
    "    self.embeddings()\n",
    "    self.loss()\n",
    "    self.optimizer()\n",
    "    self.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing the Results \n",
    "\n",
    "#### Creating a Plotting Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if mode not in potential_modes or corpus not in potential_corpora or config not in potential_configurations:\n",
    "        print(\"Please enter in a valid input as an argument\")\n",
    "        sys.exit()\n",
    "    elif mode == 'train':\n",
    "        if corpora == 'brown':\n",
    "            split_brown()\n",
    "            t_path = \"data/b_train.txt\"\n",
    "            v_path = \"data/b_valid.txt\"\n",
    "            s_path = \"data/b_test.txt\"\n",
    "        elif corpora == 'wiki':\n",
    "            t_path = \"data/w_train.txt\"\n",
    "            v_path = \"data/w_valid.txt\"\n",
    "            s_path = \"data/w_test.txt\"\n",
    "            \n",
    "        # get the configuration\n",
    "        run_configuration = potential_configurations[config]\n",
    "        # get the preprocessed training text\n",
    "        pp_tt = Preprocessor(t_path)\n",
    "        # get the size of the vocabulary\n",
    "        V = pp_tt.V\n",
    "        # training data\n",
    "        training_data = pp_tt.generate_data(t_path)\n",
    "        # validation data\n",
    "        validation_data = pp_tt.generate_data(v_path)\n",
    "        # testing data\n",
    "        testing_data = pp_tt.generate(s_path)\n",
    "        # creating the training_acc & training_cost\n",
    "        t_acc, t_loss = [.1] * 10, [7] * 10\n",
    "        # calling the model object\n",
    "        model = Bengio()\n",
    "        # calling the train_model() method \n",
    "        model.build_graph(training_data, validation_data)\n",
    "        # visualize it!\n",
    "        plot(t_acc[10:], t_loss[10:])\n",
    "        \n",
    "    elif mode == 'restore':\n",
    "        if corpora == 'brown':\n",
    "            split_brown()\n",
    "            t_path = \"data/b_train.txt\"\n",
    "            v_path = \"data/b_valid.txt\"\n",
    "            s_path = \"data/b_test.txt\"\n",
    "        elif corpora == 'wiki':\n",
    "            t_path = \"data/w_train.txt\"\n",
    "            v_path = \"data/w_valid.txt\"\n",
    "            s_path = \"data/w_test.txt\"\n",
    "        \n",
    "        # get the configuration\n",
    "        run_configuration = potential_configurations[config]\n",
    "        # get the preprocessed training text\n",
    "        pp_tt = Preprocessor(t_path)\n",
    "        # get the size of the vocabulary\n",
    "        V = pp_tt.V\n",
    "        # testing data\n",
    "        testing_data = pp_tt.generate(s_path)\n",
    "        # calling the model object\n",
    "        model = Bengio()\n",
    "        # restore the model \n",
    "        model.restore_model('../models/' + corpus + '_' + config + '.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
