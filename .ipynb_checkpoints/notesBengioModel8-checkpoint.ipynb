{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'counter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-44a18f0d5a06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'counter'"
     ]
    }
   ],
   "source": [
    "import sys # system-specific parameters and functions\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nico Notes\n",
    "\n",
    "### Preprocessor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        raw_text = read_file(path) # see helper function section below\n",
    "        top_words = Counter(raw_text).most_common()\n",
    "        words = [word[0] for word in top_words if word[1] >= 3]\n",
    "        \n",
    "        if '<unk>' in words:\n",
    "            words.remove('<unk>')\n",
    "        \n",
    "        self.word_dict = {'<unk>':0}\n",
    "        \n",
    "        for i in range(1, len(words)):\n",
    "            self.word_dict[words[i]] = i\n",
    "        self.vocab_size = len(self.word_dict)\n",
    "        self.word_dict_reverse = dict(zip(self.word_dict.values(),\n",
    "                                          self.word_dict.keys()))\n",
    "        \n",
    "        self.text_as_index = []\n",
    "        \n",
    "        for word in words:\n",
    "            idx = 0\n",
    "            if word in self.word_dict:\n",
    "                idx = self.word_dict[word]\n",
    "            self.text_as_index.append(idx)\n",
    "    \n",
    "    def generate_data(self, path):\n",
    "        words = read_file(path)\n",
    "        text_as_index = []\n",
    "        \n",
    "        for word in words:\n",
    "            idx = 0\n",
    "            if word in self.word_dict:\n",
    "                idx = self.word_dict[word]\n",
    "            text_as_index.append(idx)\n",
    "        return text_as_index            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-5-ae9886b1d79e>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-ae9886b1d79e>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    u = tf.Variable(tf.truncated_normal([self.hidden_layers, vocabulary_size].\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": [
    "class MyModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.batch_size = 256\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.window_size = config['window_size']\n",
    "        self.hidden_layers = config['hidden_units']\n",
    "        \n",
    "    def train(self, train_data, validate_data, num_epochs=6):\n",
    "        if tf.test.is_gpu_available(cuda_only=False, \n",
    "                                    min_cuda_compute_capability=None):\n",
    "            device = '/gpu:0'\n",
    "            print('Using GPU')\n",
    "        else:\n",
    "            device = '/cpu:0'\n",
    "            print('Using CPU')\n",
    "        with tf.device(device):\n",
    "            # inputs will be indexs of the n window size words b4 label\n",
    "            self.x_input = tf.placeholder(tf.int64, [None, self.window_size])\n",
    "            #labels will just be indexes of the next word\n",
    "            self.y_true = tf.placeholder(tf.int64, [None])\n",
    "            \n",
    "            # embeddings is the c function\n",
    "            embeddings = tf.Variable(tf.random_uniform([vocabulary_size, \n",
    "                                                        self.embedding_size],\n",
    "                                                      -1.0, 1.0))\n",
    "            x_flat = tf.layers.flatten(self.x_input)\n",
    "            embed = tf.nn.embedding_lookup(embeddings, x_flat)\n",
    "            \n",
    "            x_t = tf.reshape(embed, [self.batch_size,\n",
    "                                     self.window_size * self.embedding_size])\n",
    "            \n",
    "            w = tf.Variable(tf.truncated_normal([self.embedding_size * self.window_size,\n",
    "                                                 vocabulary_size],\n",
    "                                               stddev=1.0 / math.sqrt(self.embedding_size * self.window_size)))\n",
    "            b = tf.Variable(tf.random_uniform([vocabulary_size]))\n",
    "            d = tf.Variable(tf.random_uniform([self.hidden_layers]))\n",
    "            u = tf.Variable(\n",
    "                tf.truncated_normal([self.hidden_layers, vocabulary_size], stddev=1.0 / math.sqrt(vocabulary_size)))\n",
    "            h = tf.Variable(tf.truncated_normal([self.embedding_size * self.window_size, self.hidden_layers],\n",
    "                                               stddev=1.0 / math.sqrt(self.embedding_size * self.window_size)))\n",
    "            \n",
    "            # embedding is [n*b, embedding_size]\n",
    "            hidden_out = tf.nn.bias_add(tf.matmul(x_t, h), d)\n",
    "            tan_out = tf.nn.tanh(hidden_out)\n",
    "            y_logits = tf.nn.bias_add(tf.matmul(x_t, w), b) + tf.matmul(tan_out, u)\n",
    "            \n",
    "            y_pred = tf.nn.softmax(y_logits)\n",
    "            y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "            y_one_hot = tf.one_hot(self.y_true, vocabulary_size)\n",
    "            self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with-logits_v2(logits=y_logits, labels=y_one_hot))\n",
    "            \n",
    "            # Constructing the Stochastic Gradient Descent optimizer\n",
    "            learn_rate = 0.00075\n",
    "            beta1 = 0.9\n",
    "            beta2 = 0.999\n",
    "            optimizer = tf.train.AdamOptimizer(learn_rate, beta1, beta2).minimize(self.cross_entropy)\n",
    "            correct_prediction = tf.equal(y_pred_cls, self.y_true)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            \n",
    "            self.session = tf.Session()\n",
    "            self.session.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            print('Training......')\n",
    "            global acc_hist_train, cost_hist_train\n",
    "            patience = 2\n",
    "            for i in range(num_epochs):\n",
    "                batches = generate_batches(train_data, self.batch_size, self.window_size)\n",
    "                total_batches = len(batches)\n",
    "                batch_count = 0\n",
    "                last_complete = 0\n",
    "                num_messages = 10 # number of printouts per epoch\n",
    "                for batch in batches:\n",
    "                    batch_count += 1\n",
    "                    x_batch = batch[0]\n",
    "                    y_true_batch = batch[1]\n",
    "                    feed_dict_train = {self.x_input: x_batch,\n",
    "                                      self.y_true: y_true_batch}\n",
    "                    self.session.run(optimizer, feed_dict=feed_dict_train)\n",
    "                    completion = 100 * batch_count / total_batches\n",
    "                    if batch_count % (int(total_batches / num_messages)) == 0:\n",
    "                        print('Epoch #%2d-   Batch #%5d:   %4.2f %% completed.' % (i + 1, batch_count, completion))\n",
    "                        a_t, c_t = self.test(train_data)\n",
    "                        a, c = self.test(validate_data)\n",
    "                        acc_hist_train.append(a)\n",
    "                        cost_hist_train.append(c)\n",
    "                        \n",
    "                        if sum(cost_hist_train[-4:]) > sum(cost_hist_train[-8:-4]):\n",
    "                            patience = patience - 1\n",
    "                        else:\n",
    "                            patience = 2\n",
    "                        \n",
    "                        if patience == 0:\n",
    "                            print(\"Early stopping triggered\")\n",
    "                            save_path = saver.save(self.session, \"../models/\" + arg_2 + '_' + arg_3 + \".ckpt\")\n",
    "                            print(\"Model saved in path: %s\" % save_path)\n",
    "                            return\n",
    "                        \n",
    "            print('Training Completed')\n",
    "            save_path = saver.save(self.session, \"../models/\" + arg_2 + '_' + arg_3 + \".ckpt\")\n",
    "            print(\"Model saved in path: %s\" % save_path)\n",
    "            return\n",
    "    \n",
    "    def restore(self, model_path):\n",
    "        if tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None):\n",
    "            device = '/gpu:0'\n",
    "            print('Using GPU')\n",
    "        else:\n",
    "            device = '/cpu:0'\n",
    "            print('Using CPU')\n",
    "        with tf.device(device):\n",
    "            # inputs will be indexes of the n (window size) words before the label\n",
    "            self.x_input = tf.placeholder(tf.int64, [None, self.window_size])\n",
    "            # labels will just be indexes of the next word\n",
    "            self.y_true = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "            # embeddings is the c function\n",
    "            embeddings = tf.Variable(tf.random_uniform([vocabulary_size, self.embedding_size], -1.0, 1.0))\n",
    "            x_flat = tf.layers.flatten(self.x_input)\n",
    "            embed = tf.nn.embedding_lookup(embeddings, x_flat)\n",
    "            x_t = tf.reshape(embed, [self.batch_size, self.window_size * self.embedding_size])\n",
    "            w = tf.Variable(tf.truncated_normal([self.embedding_size * self.window_size, vocabulary_size],\n",
    "                                                stddev=1.0 / math.sqrt(self.embedding_size * self.window_size)))\n",
    "            b = tf.Variable(tf.random_uniform([vocabulary_size]))\n",
    "            d = tf.Variable(tf.random_uniform([self.hidden_layers]))\n",
    "            u = tf.Variable(\n",
    "                tf.truncated_normal([self.hidden_layers, vocabulary_size], stddev=1.0 / math.sqrt(vocabulary_size)))\n",
    "            h = tf.Variable(tf.truncated_normal([self.embedding_size * self.window_size, self.hidden_layers],\n",
    "                                                stddev=1.0 / math.sqrt(self.embedding_size * self.window_size)))\n",
    "\n",
    "            # embed is [n*b, embedding_size]\n",
    "            hidden_out = tf.nn.bias_add(tf.matmul(x_t, h), d)\n",
    "            tan_out = tf.nn.tanh(hidden_out)\n",
    "            y_logits = tf.nn.bias_add(tf.matmul(x_t, w), b) + tf.matmul(tan_out, u)\n",
    "\n",
    "            y_pred = tf.nn.softmax(y_logits)\n",
    "            y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "            y_one_hot = tf.one_hot(self.y_true, vocabulary_size)\n",
    "            self.cross_entropy = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_logits, labels=y_one_hot))\n",
    "    \n",
    "            # Construct the SGD Optimizer:\n",
    "            learn_rate = 0.005\n",
    "            beta1 = 0.9;\n",
    "            beta2 = 0.999\n",
    "            optimizer = tf.train.AdamOptimizer(learn_rate, beta1, beta2).minimize(self.cross_entropy)\n",
    "            correct_prediction = tf.equal(y_pred_cls, self.y_true)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            \n",
    "            # don't need extra commands here\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            with tf.Session() as sess:\n",
    "                # Restores the variables from disk.\n",
    "                saver.restore(sess, model_path)\n",
    "                print(\"Model restored.\")\n",
    "                test_batches = generate_batches(test_data, self.batch-size, self.window_size)\n",
    "                cost, acc = [], []\n",
    "                for batch in test_batches:\n",
    "                    feed_dict_test = {self.x_input: batch[0],\n",
    "                                      self.y_true: batch[1]}\n",
    "                    acc.append(sess.run(self.accuracy, feed_dict=feed_dict_test))\n",
    "                    cost.append(sess.run(self.cross_entropy, feed_dict=feed_dict_test))\n",
    "                \n",
    "                avg_acc = sum(acc) / float(len(acc))\n",
    "                avg_cost = sum(cost) / float(len(cost))\n",
    "                print(\"   Accuracy on test-set:   %4.2f %% \\n\" % (avg_acc * 100),\n",
    "                      \"   Cost on test-set:       %4.2f \\n\" % avg_cost,\n",
    "                      \"   Perplexity on test-set:       %4.2f \\n\" % np.exp(avg_cost))\n",
    "    \n",
    "    def test(self, test_data):\n",
    "        test_batches = generate_batches(test_data, self.batch_size, self.window_size)\n",
    "        cost, acc = [], []\n",
    "        for batch in test_batches:\n",
    "            feed_dict_train = {self.x_input: batch[0],\n",
    "                               self.y_true: batch[1]}\n",
    "            acc.append(self.session.run(self.accuracy, feed_dict=feed_dict_test))\n",
    "            cost.append(self.session.run(self.cross_entropy, feed_dict=feed_dict_test))\n",
    "        avg_acc = sum(acc) / float(len(acc))\n",
    "        avg_cost = sum(cost) / float(len(cost))\n",
    "        print(\"   Accuracy on valid-set:   %4.2f %%\" % (avg_acc * 100),\n",
    "              \"   Cost on valid-set:       %4.2f \\n\" % avg_cost)\n",
    "        return avg_acc, avg_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
