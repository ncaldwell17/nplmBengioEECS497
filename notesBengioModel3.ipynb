{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Multi-Layer Perceptron Neural Network Model w/ Keras\n",
    "\n",
    "Keras is designed to build models easily, and the simplest model is defined in the Sequential class - just a linear stack of layers. I can create a Sequential model and define all the layers in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the doc into memory\n",
    "def load_doc(filename):\n",
    "    # opens the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # reads all the text\n",
    "    text = file.read()\n",
    "    # closes the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# turns a document into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replaces \"--\" with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # splits into tokens by white space \n",
    "    tokens = doc.split()\n",
    "    # removes punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # removes remaining tokens that aren't alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # makes everything lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves tokens to file, one dialog per line:\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(str.append(filename, '_sequences'), 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document):\n",
    "    # loads the document\n",
    "    in_filename = document\n",
    "    doc = load_doc(in_filename)\n",
    "    # sanity check, uncomment to see first 200 characters\n",
    "    # print(doc[:200])\n",
    "    \n",
    "    # FYI, not 100% sure about how variables get returned. Keep in mind for DB\n",
    "    \n",
    "    # cleans the document\n",
    "    tokens = clean_doc(doc)\n",
    "    # sanity checks, uncomment to see first 200 tokens\n",
    "    print(tokens[:200])\n",
    "    print('Total Tokens: %d' % len(tokens))\n",
    "    print('Unique Tokens: %d' % len(set(tokens)))\n",
    "    \n",
    "    # organize into sequences of tokens\n",
    "    length = 50 + 1\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        # converts into a line\n",
    "        line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(line)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    \n",
    "    # save sequences to file\n",
    "    out_filename = '#_sequences.txt'\n",
    "    save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "document = # path to file\n",
    "# preprocesses the document and saves to file\n",
    "preprocessing(document)\n",
    "\n",
    "# loads doc into memory\n",
    "for filename in os.listdir('/data/'):\n",
    "    if filename.endswith(\"sequences.txt\"):\n",
    "        doc = load_doc(filename)\n",
    "# might need to return filename?\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenzier.texts_to_sequences(lines)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# order of the model\n",
    "n = 5\n",
    "# number of word features for MLPs\n",
    "m = 60\n",
    "# number of hidden units\n",
    "h = 50\n",
    "# vocab size \n",
    "v = idk \n",
    "# learning rate\n",
    "lr = 0.01\n",
    "# number of epochs\n",
    "e = 50\n",
    "# batch size\n",
    "b = 100\n",
    "\n",
    "# define the model \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, m, input_shape=(5,)))\n",
    "model.add(Dense(h, activation='tanh'))\n",
    "model.add(Dense(v, activation='softmax'))\n",
    "\n",
    "# sanity check\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics='accuracy')\n",
    "\n",
    "model.fit(X, y,\n",
    "          epochs=e,\n",
    "          batch_size=b,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the model\n",
    "model.save('model.h5')\n",
    "\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "# see if it prints three sets: 1) train_loss, 2) val_loss, & 3) test_loss \n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tppl = tf.exp(history_dict[train_loss])\n",
    "\n",
    "vppl = tf.exp(history_dict[val_loss])\n",
    "\n",
    "sppl = tf.exp(history_dict[test_loss])\n",
    "\n",
    "print(tppl, vppl, sppl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
