{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy\n",
    "from numpy import array\n",
    "\n",
    "from random import randint\n",
    "\n",
    "from pickle import load\n",
    "from pickle import dump \n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text\n",
    "\n",
    "Loads the doc into the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the doc into memory\n",
    "def load_doc(filename):\n",
    "    # opens the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # reads all the text\n",
    "    text = file.read()\n",
    "    # closes the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text\n",
    "\n",
    "Now, I need to transform the raw text into a sequence of tokens or words that I can use as a source to train my model. In this preprocessing step, I'll:\n",
    "1. Replace '-' with a white space so I can split words better\n",
    "2. Split words based on white space\n",
    "3. Remove all punctuation from words to reduce the vocabulary size\n",
    "4. Remove all words that aren't alphabetic to remove standalone punctuation tokens. \n",
    "5. Normalize all words to lowercase to reduce the vocabulary size.\n",
    "\n",
    "Note how I'm changing vocabulary size with each cleaning iteration. It's a big deal - a smaller vocabulary results in a smaller model that trains faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# turns a document into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replaces \"--\" with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # splits into tokens by white space \n",
    "    tokens = doc.split()\n",
    "    # removes punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # removes remaining tokens that aren't alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # makes everything lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Cleaned Text\n",
    "\n",
    "Not 100% sure why I need to do this - but this is an experiment, so I'll just go along with it. \n",
    "\n",
    "I apparently want to create sequences of 51 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves tokens to file, one dialog per line:\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open((filename + '_sequences.txt'), 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document):\n",
    "    # loads the document\n",
    "    in_filename = document\n",
    "    doc = load_doc(in_filename)\n",
    "    # sanity check, uncomment to see first 200 characters\n",
    "    # print(doc[:200])\n",
    "    \n",
    "    # FYI, not 100% sure about how variables get returned. Keep in mind for DB\n",
    "    \n",
    "    # cleans the document\n",
    "    tokens = clean_doc(doc)\n",
    "    # sanity checks, uncomment to see first 200 tokens\n",
    "    print(tokens[:200])\n",
    "    print('Total Tokens: %d' % len(tokens))\n",
    "    print('Unique Tokens: %d' % len(set(tokens)))\n",
    "    \n",
    "    # organize into sequences of tokens\n",
    "    length = 50 + 1\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        # converts into a line\n",
    "        line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(line)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    \n",
    "    # save sequences to file\n",
    "    out_filename = 'example'\n",
    "    save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Language Model\n",
    "\n",
    "The model I'm training is a neural language model -\n",
    "1. It uses a distributed representation for words so different words with similar meanings will have a similar representation.\n",
    "2. It learns the representation at the same time as learning the model\n",
    "3. It learns to predict the probability for the next word using the context of the last 100 words\n",
    "\n",
    "I will use the Embedding Layer to learn the representations of words, and a LSTM to learn to predict words based on their context. \n",
    "\n",
    "### Load Sequences\n",
    "I can load my training data by using the load_doc() method I created above. Once loaded, I'll split the data into separate training sequences by splitting based on new lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'impassioned', 'plea', 'for', 'civil', 'rights', 'created', 'a', 'landslide', 'of', 'correspondence', 'and', 'one', 'sponsor', 'even', 'asked', 'me', 'to', 'consider', 'replacing', 'the', 'eddie', 'cantor', 'comedy', 'hour', 'on', 'a', 'permanent', 'basis', 'but', 'what', 'quarter', 'could', 'a', 'poor', 'defenseless', 'woman', 'expect', 'from', 'a', 'dictator', 'who', 'would', 'even', 'make', 'so', 'bold', 'as', 'to', 'close', 'all', 'of', 'the', 'banks', 'in', 'our', 'great', 'nation', 'the', 'savage', 'barbarian', 'hordes', 'of', 'red', 'russian', 'communism', 'descended', 'on', 'the', 'athens', 'that', 'was', 'mighty', 'metronome', 'sacking', 'and', 'despoiling', 'with', 'their', 'bolshevistic', 'battle', 'cry', 'of', 'soak', 'the', 'rich', 'after', 'an', 'unspeakable', 'siege', 'lasting', 'the', 'better', 'part', 'of', 'two', 'months', 'it', 'was', 'announced', 'that', 'the', 'studio', 'owed', 'the', 'government', 'a', 'tax', 'debt', 'in', 'excess', 'of', 'eight', 'million', 'dollars', 'while', 'i', 'who', 'had', 'always', 'remained', 'aloof', 'from', 'such', 'iniquitous', 'practices', 'as', 'paying', 'taxes', 'on', 'the', 'salary', 'i', 'had', 'earned', 'and', 'the', 'little', 'i', 'legally', 'inherited', 'as', 'morris', 'helpless', 'relict', 'was', 'stung', 'with', 'a', 'personal', 'bill', 'of', 'such', 'astronomical', 'proportions', 'as', 'to', 'wipe', 'out', 'all', 'but', 'a', 'fraction', 'of', 'my', 'poor', 'hardcomeby', 'savings', 'i', 'was', 'also', 'publicly', 'reprimanded', 'dragged', 'through', 'the', 'mud', 'by', 'the', 'radical', 'press', 'and', 'made', 'a', 'figure', 'of', 'fun', 'by', 'such', 'leftist', 'publications', 'as', 'the', 'new', 'republic', 'the', 'new', 'yorker', 'time', 'and']\n",
      "Total Tokens: 14154\n",
      "Unique Tokens: 3935\n",
      "Total Sequences: 14103\n"
     ]
    }
   ],
   "source": [
    "document = 'data/brown-test.txt'\n",
    "# preprocesses the document and saves to file\n",
    "preprocessing(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads doc into memory\n",
    "in_filename = \"example_sequences.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Sequences\n",
    "The word embedding layer expects input sequences to be comprised of integers. I can map each word in my vocabulary to a unique integer and encode my input sequences. When my model will make predictions, I can convert the prediction to numbers and look up their associated words in the same mapping. \n",
    "\n",
    "To do this encoding, I'm using the Tokenizer class in the Keras API. \n",
    "\n",
    "The Tokenizer __must be trained on the entire training dataset__, which means that it finds all of the unique words in the data and assigns each a unique integer. \n",
    "\n",
    "I can then use the fit Tokenizer to encode all of the training sequences, converting each sequence from a list of words to a list of integers. \n",
    "\n",
    "I can access the mapping of words to integers as a dictionary attribute called `word_index` on the Tokenizer object. \n",
    "\n",
    "I'll need to know the size of the vocabulary for defining the embedding layer later. I can determrine the vocabulary by calculating the size of the mapping dictionary. \n",
    "\n",
    "Words are assigned values from 1 to the total number of words. __NOTE: This is what that one Keras tutorial was trying to avoid with multi-hot encoding. Pay attention to that fact when I'm either working with the model or typing my report to David__. The index of arrays is zero-offset (think: European floors). \n",
    "\n",
    "That means that when I'm specifying the vocabulary size to the Embedding layer, __I have to specify it as + 1 larger than the actual vocabulary data__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Inputs and Outputs\n",
    "\n",
    "Now that I've encoded the input sequences using Keras, I need to separate them into input (X) and ouput (y) elements. I'll do this with __array slicing__. \n",
    "\n",
    "After separating, I need to one-hot encode the output word (if you think back to my TensorFlow tutorial, this means converting it from an integer to a vector of 0 values, one for each word in the vocabulary, with a 1 to indicate the specific word at the index of the words' integer value). \n",
    "\n",
    "This is so the model learns to predict the probability distribution for the next word and the ground truth from which to learn from is 0 for all words except the actual word that comes next. \n",
    "\n",
    "`Keras` provides the `to_categorical()` method that can be used to one-hot encode the output words for each input-output sequence pair. \n",
    "\n",
    "Finally, I'll need to specify to the Embedding layer how long input sequences are. I know that there are 50 words becuase I designed the model, but its normally better to use the second dimension (i.e. the number of columns) of the input data's shape. That way, if I decide to change the length of the sequences when preparing data, I don't need to change this data loading code and it can work generically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model\n",
    "\n",
    "Now I can define and fit the language model on the training data. As inputs, the learned embedding needs to know the size of the vocabulary and the length of input sequences. It also needs a parameter to specify how many dimensions will be used to represent each word (i.e., the size of the embedding vector space). For this, common values are 50, 100, and 300. I'll use 50 here. \n",
    "\n",
    "I will use two LSTM hidden layers with 100 memory cells each. \n",
    "\n",
    "A dense fully connected layer with 100 neurons connects to the LSTM hidden layers to interpret the features extracted from the sequence. In Bengio's model, he uses a hidden layer I didn't understand and another tanh activation layer, and then a softmax. Not sure if those were dense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/noahcg/anaconda3/envs/default/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            196750    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3935)              397435    \n",
      "=================================================================\n",
      "Total params: 745,085\n",
      "Trainable params: 745,085\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Users/noahcg/anaconda3/envs/default/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "14103/14103 [==============================] - 11s 807us/step - loss: 7.2113 - acc: 0.0562\n",
      "Epoch 2/100\n",
      "14103/14103 [==============================] - 10s 702us/step - loss: 6.7133 - acc: 0.0579\n",
      "Epoch 3/100\n",
      "14103/14103 [==============================] - 10s 730us/step - loss: 6.6056 - acc: 0.0579\n",
      "Epoch 4/100\n",
      "14103/14103 [==============================] - 11s 746us/step - loss: 6.4599 - acc: 0.0579\n",
      "Epoch 5/100\n",
      " 1792/14103 [==>...........................] - ETA: 9s - loss: 6.3731 - acc: 0.0558"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-752532fcc68b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# fits the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/default/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/default/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/default/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/default/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/default/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "# sanity check \n",
    "print(model.summary())\n",
    "# FYI, training might take a couple hours without a GPU. I can\n",
    "#    speed it up with a larger batch size AND/OR fewer training\n",
    "#    epochs (see parameters below). \n",
    "    \n",
    "# compiles the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "# fits the model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model\n",
    "\n",
    "The trained model, at the end of the run, should be saved to file. I'll use the Keras API to save the model as 'model.h5' to the CWD. \n",
    "\n",
    "Later, when I load the model to make predictions, I'll also need the mapping of words to integers. This is in the Tokenizer object and I can save that using Pickle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the model\n",
    "model.save('model.h5')\n",
    "\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Language Model\n",
    "\n",
    "Now that I have all that saved, I can use it! __This isn't what the Bengio Model is supposed to return__. Instead, it returns generated new sequences of text that have the same statistical properties as the source text. This is just to demonstrate the pipeline. \n",
    "\n",
    "### Load the Data\n",
    "I can use the same code from the previous section to load the training data sequences. I need the text so that I can choose a source sequence as an input to the model to generate a new sequence of text. \n",
    "\n",
    "The model will require 100 words as an input. \n",
    "\n",
    "Later, I'll need to specify the expected length of input. __I can deterrmine this from input sequences by calculating the length of one line of the loaded data and subtracting 1 for the expected output word that is also on the same line__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the doc into memory and load the cleaned text sequences\n",
    "in_filename = #\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model\n",
    "I can also now load the model from file. \n",
    "\n",
    "Keras provides the `load_model()` function for loading the model, ready for use. I can also load the tokenizer using the Pickle API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# loads the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text\n",
    "\n",
    "The first step in generating text is __preparing a seed input__. I will select a random line of text from the input text for this purpose. Once selected, I'll print it just for a sanity check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a seed text\n",
    "seed_text = lines[randint(0, len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I can generate new words, one at a time. \n",
    "\n",
    "First, the seed text must be encoded to integers using the same tokenzier that I used when training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = tokenizer.texts_to_sequences([seed_text])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can predict the next word directly by calling `model.predict_classes()` that will return the index of the word with the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for each word\n",
    "# yhat = model.predict_classes(encoded, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can then look up the index in the Tokenizers mapping to get the associated word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_word = ''\n",
    "\"\"\"\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index == yhat:\n",
    "        out_word = word\n",
    "        break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can then append this to the seed text and repeat the process. \n",
    "\n",
    "The input sequence is __going to get too long__. I can truncate it to the desired length after the input sequence has been encoded to integers. Keras provides the `pad_sequences()` function that we can use to perform this truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to wrap all of the above functions into a single function called `generate_seq()` that takes as its input: the model, the tokenizer, the input sequence length, the seed text, and the number of words to generate. It will return a sequence of words generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    #generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as an integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to generate a sequence of new words given some seed text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
