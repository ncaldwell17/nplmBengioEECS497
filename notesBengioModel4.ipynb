{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Notes\n",
    "\n",
    "In the first three models, I was a bit unclear about what I wanted to do and in reality, I should have been combining lessons from all of them - in the first, I was trying to emulate Liam Ge's preprocessing (not realizing that he completely ignores Keras in his explanation). In the second, I was emulating Jason Brownlee's LSTM - but all that managed to do was create something to predict sequences. Now - I might be able to use his model if I can figure out if the history thing is the same as my TensorFlow classification notes and how evaluation works, cause I feel like I'm dealing with three different types of models right now: an LSTM (not what I want), a classifier (also not what I want), and an MLP (what I want), but I only know how to evaluate the first two. In the third, I was just trying to experiment with how to build an MLP without any regard for what was actually going into it. It might be helpful to build an MLP from scratch like my LSTM and just replace the parameters that I understand from the Bengio paper and use the simple `.tf.exp(history[loss])` function I found on Stack Exchange to calculate perplexity. Brownlee says that `model.evaluate` can calculate the loss values for input data, so maybe I should just build a basic MLP and then run it on a separate validation dataset and test dataset and call it a day. \n",
    "\n",
    "My main issue is that I don't really know how to do the preprocessing for an MLP. The first hypothesis is that I can copy the sequences idea from Brownlee's LSTM, but only run it on half the training corpus if Collab can't handle it? I also need to figure out how to optimize them to leverage a TPU or GPU, because running on half the training corpus might not yield the results that I want - but this can be a secondary task. \n",
    "\n",
    "Current plan: \n",
    "0. Search out MLPs from scratch [MERP]\n",
    "1. Use Brownlee's LSTM to guide preprocessing dataset and building model \n",
    "2. Run `model.evaluate()` on validation and testing. \n",
    "3. Use Tutorial's visualization and loss grabbing to get values I need. \n",
    "4. Add in extra features if you're not bored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy\n",
    "from numpy import array\n",
    "\n",
    "from random import randint\n",
    "\n",
    "from pickle import load\n",
    "from pickle import dump \n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the doc into memory\n",
    "def load_doc(filename):\n",
    "    # opens the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # reads all the text\n",
    "    text = file.read()\n",
    "    # closes the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "import string\n",
    "\n",
    "# turns a document into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replaces \"--\" with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # splits into tokens by white space \n",
    "    tokens = doc.split()\n",
    "    # removes punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # removes remaining tokens that aren't alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # makes everything lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# saves tokens to file, one dialog per line:\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open((filename + '_sequences.txt'), 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "def preprocessing_train(document):\n",
    "    # loads the document\n",
    "    in_filename = document\n",
    "    doc = load_doc(in_filename)\n",
    "    # sanity check, uncomment to see first 200 characters\n",
    "    # print(doc[:200])\n",
    "    \n",
    "    # FYI, not 100% sure about how variables get returned. Keep in mind for DB\n",
    "    \n",
    "    # cleans the document\n",
    "    tokens = clean_doc(doc)\n",
    "    # sanity checks, uncomment to see first 200 tokens\n",
    "    print(tokens[:200])\n",
    "    print('Total Tokens: %d' % len(tokens))\n",
    "    print('Unique Tokens: %d' % len(set(tokens)))\n",
    "    \n",
    "    # organize into sequences of tokens\n",
    "    length = 50 + 1\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        # converts into a line\n",
    "        line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(line)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    \n",
    "    # save sequences to file\n",
    "    out_filename = 'example'\n",
    "    save_doc(sequences, out_filename)\n",
    "    \n",
    "def preprocessing_val(document):\n",
    "    # loads the document\n",
    "    in_filename = document\n",
    "    doc = load_doc(in_filename)\n",
    "    # sanity check, uncomment to see first 200 characters\n",
    "    # print(doc[:200])\n",
    "    \n",
    "    # FYI, not 100% sure about how variables get returned. Keep in mind for DB\n",
    "    \n",
    "    # cleans the document\n",
    "    tokens = clean_doc(doc)\n",
    "    # sanity checks, uncomment to see first 200 tokens\n",
    "    print(tokens[:200])\n",
    "    print('Total Tokens: %d' % len(tokens))\n",
    "    print('Unique Tokens: %d' % len(set(tokens)))\n",
    "    \n",
    "    # organize into sequences of tokens\n",
    "    length = 50 + 1\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        # converts into a line\n",
    "        line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(line)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    \n",
    "    # save sequences to file\n",
    "    out_filename = 'example2'\n",
    "    save_doc(sequences, out_filename)\n",
    "    \n",
    "def preprocessing_test(document):\n",
    "    # loads the document\n",
    "    in_filename = document\n",
    "    doc = load_doc(in_filename)\n",
    "    # sanity check, uncomment to see first 200 characters\n",
    "    # print(doc[:200])\n",
    "    \n",
    "    # FYI, not 100% sure about how variables get returned. Keep in mind for DB\n",
    "    \n",
    "    # cleans the document\n",
    "    tokens = clean_doc(doc)\n",
    "    # sanity checks, uncomment to see first 200 tokens\n",
    "    print(tokens[:200])\n",
    "    print('Total Tokens: %d' % len(tokens))\n",
    "    print('Unique Tokens: %d' % len(set(tokens)))\n",
    "    \n",
    "    # organize into sequences of tokens\n",
    "    length = 50 + 1\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        # converts into a line\n",
    "        line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(line)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    \n",
    "    # save sequences to file\n",
    "    out_filename = 'example3'\n",
    "    save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Language Model\n",
    "\n",
    "Unlike in my second series of notes, I will use the embedding layer to learn the representations of words, and then two Dense layers with the requisite activation functions and see what happens. \n",
    "\n",
    "### Loading the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', 'atlantas', 'recent', 'primary', 'election', 'produced', 'no', 'evidence', 'that', 'any', 'irregularities', 'took', 'place', 'the', 'jury', 'further', 'said', 'in', 'termend', 'presentments', 'that', 'the', 'city', 'executive', 'committee', 'which', 'had', 'overall', 'charge', 'of', 'the', 'election', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'city', 'of', 'atlanta', 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', 'the', 'septemberoctober', 'term', 'jury', 'had', 'been', 'charged', 'by', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'to', 'investigate', 'reports', 'of', 'possible', 'irregularities', 'in', 'the', 'hardfought', 'primary', 'which', 'was', 'won', 'by', 'mayornominate', 'ivan', 'allen', 'jr', 'only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', 'the', 'jury', 'said', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', 'the', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', 'georgias', 'registration', 'and', 'election', 'laws', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', 'it', 'recommended', 'that', 'fulton', 'legislators', 'act', 'to', 'have', 'these', 'laws', 'studied', 'and', 'revised', 'to', 'the', 'end', 'of', 'modernizing', 'and', 'improving', 'them', 'the', 'grand', 'jury', 'commented', 'on', 'a', 'number', 'of', 'other', 'topics', 'among', 'them', 'the', 'atlanta', 'and', 'fulton', 'county', 'purchasing', 'departments', 'which', 'it', 'said', 'are', 'well', 'operated', 'and', 'follow', 'generally', 'accepted', 'practices', 'which', 'inure', 'to', 'the', 'best', 'interest', 'of']\n",
      "Total Tokens: 269393\n",
      "Unique Tokens: 22462\n",
      "Total Sequences: 269342\n"
     ]
    }
   ],
   "source": [
    "document = 'data/brown-train.txt'\n",
    "# preprocesses the document and saves to file\n",
    "preprocessing_train(document)\n",
    "\n",
    "# loads doc into memory\n",
    "in_filename = \"example_sequences.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I Changed\n",
    "\n",
    "Here, I replaced the Embedding vector space with the dimmensions that I thought Bengio noted for his MLPs (as the second argument for Embedding()), I removed the LSTMs, and changed the activation for the first dense layer to tanh. I also changed the first argument of Dense to 50 - because I think that's the hidden units.\n",
    "\n",
    "I'm winging it on the epochs - I can change it to something higher if the accuracy I get makes it seem like I low balled it. \n",
    "\n",
    "If it takes a while to train, I'll cut the batch size in half. Likewise, I might throw more in there if I can. \n",
    "\n",
    "I don't really know how to implement the mix that he mentions, or the order, or the direct - so this is as good as it's going to get for a bit.I can always ask David about that later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/noahcg/anaconda3/envs/default/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 60)            1347780   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           64400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 22463)             1145613   \n",
      "=================================================================\n",
      "Total params: 2,643,243\n",
      "Trainable params: 2,643,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Users/noahcg/anaconda3/envs/default/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "269342/269342 [==============================] - 408s 2ms/step - loss: 7.4037 - acc: 0.0683\n",
      "Epoch 2/25\n",
      "269342/269342 [==============================] - 423s 2ms/step - loss: 7.2213 - acc: 0.0683\n",
      "Epoch 3/25\n",
      "269342/269342 [==============================] - 419s 2ms/step - loss: 7.2140 - acc: 0.0683\n",
      "Epoch 4/25\n",
      "269342/269342 [==============================] - 420s 2ms/step - loss: 7.2087 - acc: 0.0683\n",
      "Epoch 5/25\n",
      "269342/269342 [==============================] - 414s 2ms/step - loss: 7.2055 - acc: 0.0683\n",
      "Epoch 6/25\n",
      "269342/269342 [==============================] - 585s 2ms/step - loss: 7.2046 - acc: 0.0683\n",
      "Epoch 7/25\n",
      "269342/269342 [==============================] - 424s 2ms/step - loss: 7.2006 - acc: 0.0683\n",
      "Epoch 8/25\n",
      "269342/269342 [==============================] - 449s 2ms/step - loss: 7.2001 - acc: 0.0683\n",
      "Epoch 9/25\n",
      "269342/269342 [==============================] - 431s 2ms/step - loss: 7.1987 - acc: 0.0683\n",
      "Epoch 10/25\n",
      "269342/269342 [==============================] - 412s 2ms/step - loss: 7.1962 - acc: 0.0683\n",
      "Epoch 11/25\n",
      "269342/269342 [==============================] - 400s 1ms/step - loss: 7.1948 - acc: 0.0683\n",
      "Epoch 12/25\n",
      "269342/269342 [==============================] - 396s 1ms/step - loss: 7.1941 - acc: 0.0683\n",
      "Epoch 13/25\n",
      "269342/269342 [==============================] - 398s 1ms/step - loss: 7.1931 - acc: 0.0683\n",
      "Epoch 14/25\n",
      "269342/269342 [==============================] - 404s 2ms/step - loss: 7.1920 - acc: 0.0683\n",
      "Epoch 15/25\n",
      "269342/269342 [==============================] - 395s 1ms/step - loss: 7.1929 - acc: 0.0683\n",
      "Epoch 16/25\n",
      "269342/269342 [==============================] - 395s 1ms/step - loss: 7.1914 - acc: 0.0683\n",
      "Epoch 17/25\n",
      "269342/269342 [==============================] - 393s 1ms/step - loss: 7.1899 - acc: 0.0683\n",
      "Epoch 18/25\n",
      "269342/269342 [==============================] - 414s 2ms/step - loss: 7.1899 - acc: 0.0683\n",
      "Epoch 19/25\n",
      "269342/269342 [==============================] - 425s 2ms/step - loss: 7.1901 - acc: 0.0683\n",
      "Epoch 20/25\n",
      "269342/269342 [==============================] - 423s 2ms/step - loss: 7.1894 - acc: 0.0683\n",
      "Epoch 21/25\n",
      "269342/269342 [==============================] - 411s 2ms/step - loss: 7.1898 - acc: 0.0683\n",
      "Epoch 22/25\n",
      "269342/269342 [==============================] - 417s 2ms/step - loss: 7.1900 - acc: 0.0683\n",
      "Epoch 23/25\n",
      "108544/269342 [===========>..................] - ETA: 4:08 - loss: 7.1856 - acc: 0.0686"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 60, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# sanity check \n",
    "print(model.summary())\n",
    "    \n",
    "# compiles the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "# fits the model\n",
    "history = model.fit(X, y, batch_size=128, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the model\n",
    "model.save('model.h5')\n",
    "\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements Taken from TF Tutorial \n",
    "\n",
    "I'm just now noticing that the model I did in the tutorials explicitly separates the training and validation data and reports the accuracy and loss for both of them while training. I might see if I can play around with that after I finish the first hypothesis. \n",
    "\n",
    "In order to get the history, I grabbed it from a variable I created to house the model. TBD if this interferes with anything else. \n",
    "\n",
    "It also looks like I might have to format the validation data and the test data in the same way as I formatted the training data in order to get it to work properly. Keep that in mind below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = 'data/brown-val.txt'\n",
    "# preprocesses the document and saves to file\n",
    "preprocessing_val(document)\n",
    "\n",
    "# loads doc into memory\n",
    "in_filename = \"example2_sequences.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenzier.texts_to_sequences(lines)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "vX, vy = sequences[:,:-1], sequences[:,-1]\n",
    "vy = to_categorical(vy, num_classes=vocab_size)\n",
    "seq_length = vX.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_results = model.evaluate(vX, vy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = 'data/brown-test.txt'\n",
    "# preprocesses the document and saves to file\n",
    "preprocessing_test(document)\n",
    "\n",
    "# loads doc into memory\n",
    "in_filename = \"example3_sequences.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenzier.texts_to_sequences(lines)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "tX, ty = sequences[:,:-1], sequences[:,-1]\n",
    "ty = to_categorical(ty, num_classes=vocab_size)\n",
    "seq_length = tX.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.evaluate(tX, ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing and Calculating Perplexity \n",
    "\n",
    "After I run `model.fit()`, it returns a `history` object that contains a dictionary with everything that happened during training. There are currently only two entires for each monitored metric. The following code makes sure that these are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = val_results.history\n",
    "val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = test_results.history\n",
    "test_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['acc']\n",
    "loss = history_dict['loss']\n",
    "val_acc = val_dict['acc']\n",
    "val_loss = val_dict['loss']\n",
    "test_acc = test_dict['acc']\n",
    "test_loss = test_dict['loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "# r+ is for \"solid red pluses\"\n",
    "plt.plot(epochs, test_loss, 'r+', label='Test Loss')\n",
    "plt.title('Training, Validation, and Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tppl = tf.exp(history_dict[loss])\n",
    "\n",
    "vppl = tf.exp(val_dict[loss])\n",
    "\n",
    "sppl = tf.exp(test_dict[loss])\n",
    "\n",
    "print(tppl, vppl, sppl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
