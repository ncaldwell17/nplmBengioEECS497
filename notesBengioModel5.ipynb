{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengio's Model Implementation\n",
    "\n",
    "In this model, I need to achieve three things:\n",
    "- associate each word in the vocabulary given with a distributed word feature vector. \n",
    "- express the joint probability funciton of word sequences in terms of the feature vectors of these words in a sentence (i.e. generate sequences like the LSTM model)\n",
    "- learn the word feature vectors and the parameters of the probability function through a multi-layer perceptron. \n",
    "\n",
    "I also need to learn how to properly optimize a model to use TPUs or GPUs so the training time gets reduced, and figure out a method to calculate perplexity. \n",
    "\n",
    "__An overview of the TPU conversion workflow__:\n",
    "1. Build a Keras model for training in functional API with static input `batch_size`\n",
    "2. Convert Keras model to TPU model\n",
    "3. Train the TPU model with static `batch_size * 8` and save the weights to file. \n",
    "4. Build a Keras model for inference with the same structure but variable batch input size. \n",
    "5. Load the model weights\n",
    "6. Predict with the inferencing model \n",
    "7. Activate the TPU in Colab after uploading directory to Drive and Mounting it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Input, LSTM, Bidirectional, Dense, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Input Batch Size\n",
    "\n",
    "Input pipelines running on CPU and GPU are mostly free from static shape requirements, while in a TPU environment, static shapes and batch sizes are imposed. \n",
    "\n",
    "The TPU is not fully utilized unless all 8 TPU cores are used. To fully speed up trianing, I can choose a larger batch size compared to training the same model on a single GPU. A batch size of 1024 (i.e. 128 per core) is generally a good start point. \n",
    "\n",
    "__In Keras__, to define a static batch size, I use its functional API and then specify the `batch_size` parameter for the input layer. The model builds in a function which takes a `batch_size` parameter so I can come back later to make another model for inferencing runs on CPU or GPU which take variable batch size inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'maxlen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5124f9004917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtraining_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-5124f9004917>\u001b[0m in \u001b[0;36mmake_model\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     source = Input(shape=(maxlen,), batch_size=batch_size,\n\u001b[0m\u001b[1;32m      3\u001b[0m                    dtype=tf.int32, name='Input')\n\u001b[1;32m      4\u001b[0m     embedding = Embedding(input_dim=max_features,\n\u001b[1;32m      5\u001b[0m                           output_dim=128, name='Embedding')(source)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'maxlen' is not defined"
     ]
    }
   ],
   "source": [
    "def make_model(batch_size=None):\n",
    "    source = Input(shape=(maxlen,), batch_size=batch_size,\n",
    "                   dtype=tf.int32, name='Input')\n",
    "    embedding = Embedding(input_dim=max_features,\n",
    "                          output_dim=128, name='Embedding')(source)\n",
    "    lstm = LSTM(32, name='LSTM')(embedding)\n",
    "    predicted_var = Dense(1, activation='sigmoid', name='Output')(lstm)\n",
    "    model = tf.keras.Model(inputs=[source], outputs=[predicted_var])\n",
    "    model.compile(\n",
    "        optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "training_model = make_model(batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
