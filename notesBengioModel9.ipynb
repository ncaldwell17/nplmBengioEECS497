{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this iteration, I want to map out all the variables described in Bengio's Neural Probabilistic Language Model and focus entirely on the `class Model()` architecture. The full class will be defined at the bottom, with each individual attribute and method explained in detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Four things need to be defined in the initialization that I won't get from the data preprocessor that are either standard variables or derived from a specific version of the model implemented on the Brown Corpus:\n",
    "1. the batch_size (standard across all configurations)\n",
    "2. embedding_size (dependent on number of word features in a specific configuration (i.e. m on pg. 1149 of Bengio) \n",
    "3. window_size (dependent on the order of the model in a specific configuration (i.e. n on pg. 1149 of Bengio). I.e. this determines the [n] in __n__-gram. \n",
    "4. hidden units (i.e. h on pg. 1149 of Bengio, and dependent on the configuration chosen. \n",
    "\n",
    "__Nico's route__: Nico did initialization by making each of the variables a key to a value as a value within another dictionary where the key is the configuration name - then just extracts the value 2 dictionaries in. All of the variables become attributes of the model upon initialization. The variables are automatically initialized when a method of the MyModel class is called __(not entirely sure how the configs get in there b/c nothing gets passed into any of the methods that relates to them, so its magic for now)__\n",
    "\n",
    "__On Batch Sizes__: They're needed for the stochastic gradient descent. Good batch sizes are typically 32, 64, 128, or 256. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    self.window_size = user_type['window_size']\n",
    "    self.embedding_size = user_type['embedding_size']\n",
    "    self.hidden_units = user_type['hidden_units']\n",
    "    self.batch_size = 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This is the most complicated part of the entire model. I've broken it down into two major phases, the second phase being so exhaustive that it needs to be broken down itself into seven steps.\n",
    "\n",
    "`def train():` takes in the following arguments:\n",
    "* `self` allows the attributes assigned in the `__init__()` method above to be used by the method.\n",
    "* `train_data` the result derived from the `corpus.generate_data(path)` method, which is a method of the Preprocessor class that can be linked to a `corpus` object after its assigned by calling Preprocessor on a path to the data. It takes the form of a __list__.\n",
    "* `validation_data`, result derived from the same process as above, just with a link to a different path. It takes the form of a __list__. \n",
    "* `number_of_epochs=int`, a manually set integer indicating how many epochs to run. Because of the size of the data, epochs aren't the smallest data input but are __instead split into batches that have their own comamnds altering them__. \n",
    "\n",
    "### Step One - If Statements to Determine Runing on GPU/CPU/TPU\n",
    "In this step, the program makes use of the following methods:\n",
    "1. `tf.test.is_gpu_available()`\n",
    "    * This method accepts two arguments:\n",
    "        - cuda_only=__True/False__ = _limit the search to CUDA GPUs_\n",
    "        - min_cuda_compute_capability=__None__ = _a major/minor pair that indicates the minimum CUDA compute capability required, or None if no requirement.\n",
    "    * This method returns a boolean that indicates whether or not a GPU device of the requested kind is available. \n",
    "2. `tf.device()`\n",
    "    * This method allows me to __manually__ place my device instead of tf automatically assigning things. \n",
    "    * This method accepts an argument: device. \n",
    "        - device is a __string__, that will always take the following forms:\n",
    "            * '/cpu:0'\n",
    "            * '/gpu:0' (<-- The integer will change if I have another GPU available through my machine, in the same way a list in programming works (i.e. 0 = 1, 1 = 2, etc.)\n",
    "            \n",
    "3. `with tf.device(device):`\n",
    "    * This indicates that everything indented after this statement will run on the intended machine. \n",
    "    \n",
    "__NOTE:__ Not having Step One implemented was the reason that none of my training speeds were actually going faster with Google Colab. My program was basically pretty much just ignoring that a GPU existed, even if I had selected the program to run with a GPU (all that selection did was make it available for the program to use, I actually had to hard code a command for my program to take advantage of it. \n",
    "\n",
    "I still have a bit of confusion here - the TF documentation indicates that certain operations will give priority to certain operations if a GPU is made available. Still don't understand why my programs were still being obnoxiously slow if this was the case - maybe I just didn't use an operation that gave it priority - hence what I was missing being the lack of manual placement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(self, train_data, validation_data, number_of_epochs=5):\n",
    "    # insert program here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-4-107104b403ff>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-107104b403ff>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    # everything I want to run using the device indented here\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "if tf.is_gpu_available(cuda_only=False,\n",
    "                       min_cuda_compute_capability=None):\n",
    "    device = '/gpu:0'\n",
    "    print(\"Currently using GPU capabilities\")\n",
    "# here I'll figure out how to make it use a TPU\n",
    "else: \n",
    "    device = '/cpu:0'\n",
    "    print('No GPU available, using CPU capabilities')\n",
    "with tf.device(device):\n",
    "    # everything I want to run using the device indented here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Two - Everything Else But Saving the Model\n",
    "\n",
    "Here, I have to split the remaining part into six parts because they __all happen with the device running__. In this part, I'll assign, define, and describe:\n",
    "1. Placeholders\n",
    "2. The Model's Variables\n",
    "3. The Hidden Layer\n",
    "4. The Softmax\n",
    "5. Stochastic Gradient Descent Optimizer\n",
    "6. The Compiler (bit more complicated than how Keras makes it) \n",
    "\n",
    "#### 2.1 = The Placeholders\n",
    "_What are placeholders in TensorFlow?_ A placeholder is just a variable that I'll asign data to at a later date. It allows me to create operations and begin to build the computation graph without actually needing the data itself. Useful when I'm creating the model as a class instead of actually passing it the data beforehand. \n",
    "\n",
    "_What does `tf.placeholder()` take as its arguments?_\n",
    "* This method takes three arguments:\n",
    "        1. data_type = a `tf.attribute` such as `tf.int64`\n",
    "        2. shape = [optional] the shape (i.e. a list of len=2 with the # of total x's, and the # of total y's) to be fed.\n",
    "        3. name = [optional] a name for the operation. \n",
    "\n",
    "_What are the placeholders I need to create?_ Right now, I need two placeholders:\n",
    "1. x_input = indexes of the window-size words before the label\n",
    "2. y_input = indexes of the next word\n",
    "\n",
    "_Where does Bengio actually talk about these in his paper?_ Who the fuck knows. __CHECKUP: I'll have to run the total program in PyCharm to see what they produce__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need my test function to be able to acccess this outside of train()\n",
    "self.x_input = tf.placeholder(tf.int64, [None, self.window_size])\n",
    "# Not sure why I don't need to feed in the shape here\n",
    "self.y_input = tf.placeholder(tf.int64, [None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 = The Variables\n",
    "\n",
    "_What are all the variables that Bengio discusses in his discussion of the model?_ Bengio begins his discussion of his model on Pg. 1141 of his paper, while the exact variables are discussed on Pg. 1143\n",
    "\n",
    "__The Set-Up__:\n",
    "* z = simplifying a common multipication of embed_size and window_size\n",
    "* x_flat = ??\n",
    "* embed = ??\n",
    "* x_t = ??\n",
    "\n",
    "__The Free-Parameters:__\n",
    "* C = word features/embeddings\n",
    "* b = output biases\n",
    "* d = hidden layer biases \n",
    "* W = word features to output weights\n",
    "* U = hidden-to-output weights\n",
    "* H = hidden layer weights\n",
    "\n",
    "__The New `tf.methods`__:\n",
    "- `tf.Variable()` = adding a variable to graph. \n",
    "\n",
    "__Note:__ Page 1143 explicitly lays out what each of the free parameters are, though I found it difficult to ascertain what exactly the set up variables meant. I also found it very difficult to plot the exact way of representing all of the calculations in TensorFlow/python - required a lot of tutoring and careful walking through to get to that point. \n",
    "\n",
    "_I keep hearing about a TensorFlow dataflow graph, explain more about that._ __Dataflow__ is a common programming model for parallel computing. In a dataflow graph, the nodes represent units of ccomputation and the endges represent data consumed or produced by a computtation. If I look at the diagram [here](https://www.tensorflow.org/guide/graphs), I can see that each operation (like `tf.matmul` has two incoming edges (edges meaning the arrows that flow into the node) symbolizing their two inputs, and one outgoing edge (i.e. the output) symbolizing the result. The DataFlow graph makes it easier to visualize these operations. \n",
    "\n",
    "_That's cool - but there's a thing called `tf.graph` that seems different_ Yes, `tf.graph` is a bit more technical. A single `tf.graph` instancec contains two types of information:\n",
    "1. Graph strucutre = the nodes and edges of the graph, indicating how individual operations are composed together but not talking about how they should be used. The graph structure is __like assembly code__ inspecting it can convey useful information but doesn't ccontain context the same way that source code does. \n",
    "2. Graph collections = collections of metadata associated with a bunch of informative methods like `tf.add_to_collection()` which enables me to associate a list of objects with a key (where `tf.GraphKeys` defines some of the standard keys and `tf.get_collection` enables me to look up all objects associated with a key. \n",
    "\n",
    "_Awesome, but I still don't get how that connects to the actual stuff I'm writing, that still sounds really abstract_ Yep, let's just keep diving into it. Many parts of the TensorFlow library use the `tf.graph` facility. When I create a `tf.Variable`, it's added by default to collections representing \"global variables\" and \"trainable variables\". When I later come to create a `tf.train.Saver` or `tf.train.Optimizer`, the variables stored in these collections are used as the default arguments.\n",
    "\n",
    "_Alright, so how do I build a graph? And I still don't really understand how it fits into a model construction workflow?_ No problem - most TensorFlow programs start with a dataflow graph construction phase. You don't realize you're doing it because it's done automatically through other TensorFlow API methods that construct new `tf.Operation` (i.e. Nodes) and `tf.Tensor` (edges) objects and add them to a `tf.graph` instance. This is all implicit within the API - unfortunate if you want to get a handle on what's going on underneath the surface, but rest easy: you can get a sense of what's happening when you call these auxillary functions.\n",
    "    * executing something like what I'm doing below (i.e. `v = tf.Variable()`) adds a `tf.Operation` that stores a writeable tensor value that can be used like a tensor. __This is what I'm saying - get a sense of what's happening__. Ask yourself, what can a tensor do? Having a tensor object lets you use TensorFlow operations. This is great! Most TF operations take one or more `tf.Tensor` objects as arguments. \n",
    "    \n",
    "_Cool, so I know how to create these things now. What do I do with them?_ That's the next section. For now, I'll focus on the code implementation of the abstract math that's on pg. 1143 of the Bengio model.\n",
    "\n",
    "- I'm pretty sure that `tf.reshape()` and `tf.truncated_normal` do the same thing. \n",
    "\n",
    "- `tf.shape` a tf shape is basically just a description of a matrix (i.e. `Shape.create(2, 3)` will create a 2 X 3 matrix. \n",
    "\n",
    "- `tf.random_uniform()` = the bread and butter of creating tf variables. It takes the following arguments:\n",
    "    * shape = a 1-D integer Tensor or  Python array. The shape of the output tensor. \n",
    "    * minval = a 0-D Tensor or  Python value of the same type. Essentially the lower bound on the range of random values to generate. \n",
    "    * maxval = same, but for the upper bound. \n",
    "    * dtype (optional) \n",
    "    * seed (random seed) \n",
    "    * name = name for the operation \n",
    "- `tf.flatten()` = flattens an input tensor while preserving the batch axis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seemed to need this repetitively \n",
    "z = self.embedding_size * self.window_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating The Free Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tf.Variable(tf.random_uniform([self.hidden_units]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.Variable(tf.random_uniform([vocab_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The C Matrix\n",
    "Now I'll do the biases - first with the word features matrix. This is the most complicated parameter to make and understand, becuase it requires four different operations to get to the point where it can be used in the model. \n",
    "\n",
    "First, I need to create a TF variable called word_embeddings to represent a matrix of shape [vocab_size, embedding_size] with the lower bound being -1.0 and upper bound being 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b148cbbdbb25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m word_embeddings = tf.Variable(tf.random_uniform([vocab_size, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                  self.embedding_size],\n\u001b[1;32m      3\u001b[0m                                                 \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                 1.0))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "word_embeddings = tf.Variable(tf.random_uniform([vocab_size, \n",
    "                                                 self.embedding_size],\n",
    "                                                -1.0,\n",
    "                                                1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I need to multiply the additional parameters of x_input. \n",
    "\n",
    "__CHECKUP__: I'm not really sure why I need to do this - it seems like it just multiplies any additional axis together to make a none X 1 matrix and I thought my self.x_input was already that. I'll check the values when I have values to experiment with.\n",
    "\n",
    "_GUESS_: It might be that the `.layers` does something unique here too. I'll check the difference later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flat = tf.layers.flatten(self.x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll need to take my newfound x_flat elements and word_embeddings and use:\n",
    "\n",
    "`tf.nn.embedding_lookup()` = returns a `Tensor` with the same type as the tenors in `params`. It takes the following as arguments:\n",
    "* `params` = A single tensor representing the complete embedding tensor, or a list of P tensors all of the same shape except for the first dimension, representing sharded embedding tensors. \n",
    "* `ids` = A `Tensor` with type `int64` or something similar containing the IDs  to be looked up in params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = tf.nn.embedding_lookup(word_embeddings, x_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'll reshape the lookup result with a matrix of the batch_size and the word features (m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = tf.reshape(lookup, [self.batch_size, z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### H\n",
    "\n",
    "H is the hidden layer weights found by taking a matrix of [ |V| x (n - 1)m]. \n",
    "\n",
    "In creating this weight, I'm going to use:\n",
    "`tf.truncated_normal(arg1, arg2, arg3)` which returns generated values that follow a normal distribution with specified mean and standard deviation. Values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked. This method takes in the following as arguments:\n",
    "- `shape` a 1D integer Tensor or Python array that is the shape of the output tensor. \n",
    "- `mean` a  0-D Tensor or Python value of type dtype. It will be the mean of the truncated normal distribution. \n",
    "- `stddev` a 0-D Tensor or Python value of type dtype. It will be the standard deviation of the normal distribution before truncation. I can skip this if I divide the stddev by the sqrt of the divisor.  \n",
    "\n",
    "Weights are essential to get artificial neurons to learn. If the weight falls outside your truncated normal distribiution, the neuron __won't learn__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = tf.Variable(tf.truncated_normal([z, self.hidden_units], \n",
    "                                    (stddev=1.0 / math.sqrt(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  W \n",
    "\n",
    "W is the word-features to output weight found by taking a matrix of [ |V| x (n - 1)m].\n",
    "\n",
    "I will also be using `tf.truncated_normal()` to create this weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal([z, vocab_size], \n",
    "                                    (stddev=1.0 / math.sqrt(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### U \n",
    "\n",
    "U is the hidden-to-output weight found by taking a matrix [ |V| x h]. \n",
    "\n",
    "I will also be using `tf.truncated_normal()` to create this weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = tf.Variable(tf.truncated_normal([self.hidden_layers, vocab_size],\n",
    "                                    (stddev=1.0 / vocab_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 - Hidden Layer \n",
    "\n",
    "For this layer, I'll be using:\n",
    "\n",
    "`tf.nn.bias_add()` =\n",
    "\n",
    "AND\n",
    "\n",
    "`tf.matmul()` = \n",
    "\n",
    "AND\n",
    "\n",
    "`tf.tanh()` = \n",
    "\n",
    "\n",
    "This layer is referenced on Pg. 1142 of Bengio's paper, \"therefore there are really two hidden layers, the shared word features layer C (xt below), which has no non-linearity, and the ordinary hyperbolic tangent layer with input being h1_output. \n",
    "\n",
    "Unnormalized log-probabilities (y) for each output word i is computed with the third command. The equation is seen in the first paragraph of Pg. 1143 of Bengio's paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layers\n",
    "# WAR\n",
    "tanh = tf.nn.tanh(tf.nn.bias_add(tf.matmul(xt, H), d))\n",
    "y = tf.nn.bias_add(tf.matmul(xt, W), b) + tf.matmul(tanh_output, U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Softmax \n",
    "\n",
    "Here, I'll be using:\n",
    "\n",
    "`tf.nn.softmax()` = computes softmax activations (i.e. divides the exponential of a logit by the reduced sum of those same logits). Takes an input of a vector of K real numbers and normalizes it into a probability distribution.  \n",
    "\n",
    "`tf.math.argmax()` = returns the index with the largest value across axes of a tensor. Used as a way of measuring accuracy of result in the optimizer. \n",
    "\n",
    "`tf.one_hot()` = \n",
    "\n",
    "`tf.math.reduce_mean()` = computes the mean of elements across dimensions of a tensor. \n",
    "\n",
    "`tf.nn.softmax_cross_entropy_with_logits_v2` = Measures the probability error in discrete classification tasks. Takes the following arguments:\n",
    "* arg1 = __labels__ = valid probability distribution \n",
    "* arg2 = __logits__ = unscaled log probabilities \n",
    "\n",
    "__NOTE:__ In all honesty, I had no idea I was supposed to use softmax cross entropy with logits. I was kind of winging it with softmax cross entropy up to this point. \n",
    "\n",
    "The goal of this section is to create the arguments to feed into the `tf.nn.softmax_cross_entropy_with_logits_v2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probdist = tf.nn.softmax(y)\n",
    "y_ideal = tf.math.argmax(y_probdist, axis=1)\n",
    "# produces labels to use in the softmax_cross_entropy_with_logits\n",
    "y_labels = tf.one_hot(self.y_input, vocab_size)\n",
    "# I want other functions to be able to access the result (i.e. self)\n",
    "# WAR\n",
    "self.ce_result = tf.math.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(y_labels, y_probdist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Stochastic Gradient Ascent Optimizer\n",
    "\n",
    "Bengio names his optimizer on Pg. 1143 in his discussion of SGA. Gradient _ascent_ differs from _descent_ by aiming to maximize some objective function. \n",
    "\n",
    "Constructing an optimizer needs a learn rate, which is mentioned on Pg. 1147 as ε_o = 10−3.\n",
    "\n",
    "To build the optimizer, I just went off of what TF had in their [instructions](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) for the Adam Optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the SGA \n",
    "# WAR WAR WAR\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "adam = tf.train.AdamOptimizer(learning_rate, beta1, beta2).maximize(self.ce_result)\n",
    "ra = tf.equal(y_ideal, self.y_input)\n",
    "self.accuracy = tf.math.reduce_mean(tf.cast(ra, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.session = tf.Session()\n",
    "self.session.run(tf.global_variables_initalizer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.batch_size = 256 # could be changed \n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.window_size = config['window_size']\n",
    "        self.hidden_layers = config['hidden_units']\n",
    "        \n",
    "\n",
    "    def train(self, t_data, v_data, number_of_epochs=5):\n",
    "        # insert rest of program here\n",
    "        if tf.is_gpu_available(cuda_only=False,\n",
    "                           min_cuda_compute_capability=None):\n",
    "        device = '/gpu:0'\n",
    "        print(\"Currently using GPU capabilities\")\n",
    "        # here I'll figure out how to make it use a TPU\n",
    "        else: \n",
    "            device = '/cpu:0'\n",
    "            print('No GPU available, using CPU capabilities')\n",
    "        with tf.device(device):\n",
    "        # everything I want to run using the device indented here \n",
    "            # I need my test function to be able to acccess this outside of train()\n",
    "            self.x_input = tf.placeholder(tf.int64, [None, self.window_size])\n",
    "            # Not sure why I don't need to feed in the shape here\n",
    "            self.y_input = tf.placeholder(tf.int64, [None])\n",
    "            \n",
    "            # seemed to need this repetitively \n",
    "            z = self.embedding_size * self.window_size\n",
    "            \n",
    "            # hidden layer biases\n",
    "            d = tf.Variable(tf.random_uniform([self.hidden_units]))\n",
    "            # output biases\n",
    "            b = tf.Variable(tf.random_uniform([vocab_size]))\n",
    "            \n",
    "            # weights\n",
    "            # C function \n",
    "            word_embeddings = tf.Variable(tf.random_uniform([vocab_size, \n",
    "                                                 self.embedding_size],\n",
    "                                                -1.0,\n",
    "                                                1.0))\n",
    "            x_flat = tf.layers.flatten(self.x_input)\n",
    "            lookup = tf.nn.embedding_lookup(word_embeddings, x_flat)\n",
    "            xt = tf.reshape(lookup, [self.batch_size, z])\n",
    "            \n",
    "            # H\n",
    "            H = tf.Variable(tf.truncated_normal([z, self.hidden_units], \n",
    "                                    (stddev=1.0 / math.sqrt(z)))\n",
    "            # W\n",
    "            W = tf.Variable(tf.truncated_normal([z, vocab_size], \n",
    "                                    (stddev=1.0 / math.sqrt(z)))\n",
    "            # U\n",
    "            U = tf.Variable(tf.truncated_normal([self.hidden_units, vocab_size],\n",
    "                                    (stddev=1.0 / vocab_size)))\n",
    "                            \n",
    "            # hidden layers\n",
    "            # WAR\n",
    "            tanh = tf.nn.tanh(tf.nn.bias_add(tf.matmul(xt, H), d))\n",
    "            y = tf.nn.bias_add(tf.matmul(xt, W), b) + tf.matmul(tanh, U)\n",
    "            \n",
    "            # softmax \n",
    "            y_probdist = tf.nn.softmax(y)\n",
    "            Y_ideal = tf.math.argmax(y_probdist, axis=1)\n",
    "            # produces labels to use in the softmax_cross_entropy_with_logits\n",
    "            y_labels = tf.one_hot(self.y_input, vocab_size)\n",
    "            # I want other functions to be able to access the result (i.e. self)\n",
    "            # WAR\n",
    "            self.ce_result = tf.math.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(y_labels, y_probdist))\n",
    "            \n",
    "            # building the SGA \n",
    "            # WAR WAR WAR\n",
    "            learning_rate = 0.001\n",
    "            beta1 = 0.9\n",
    "            beta2 = 0.999\n",
    "            adam = tf.train.AdamOptimizer(learning_rate, beta1, beta2).maximize(self.ce_result)\n",
    "            ra = tf.equal(y_ideal, self.y_input)\n",
    "            self.accuracy = tf.math.reduce_mean(tf.cast(ra, tf.float32))\n",
    "                            \n",
    "            self.session = tf.Session()\n",
    "            self.session.run(tf.global_variables_initalizer())\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            print('training beginning . . . . .')\n",
    "            global accuracy_hist_train, cost_hist_train\n",
    "            for i in range(number_of_epochs):\n",
    "                batches = generate_batches(train_data, \n",
    "                                           self.batch_size, \n",
    "                                           self.window_size)\n",
    "                total_batches = len(batches)\n",
    "                batch_count = 0\n",
    "                last_complete = 0\n",
    "                num_messages = 10 # the number of  printouts  per  epoch\n",
    "                for batch in batches:\n",
    "                    batch_count += 1\n",
    "                    x_batch = batch[0]\n",
    "                    y_batch = batch[1]\n",
    "                    feed_dict_train = {self.x_input: x_batch,\n",
    "                                       self.y_input: y_batch}\n",
    "                    self.session.run(optimizer, feed_dict=feed_dict_train)\n",
    "                    completion = 100 * batch_count / total_batches\n",
    "                    if batch_count % (int(total_batches / num_messages)) == 0:\n",
    "                        print('Epoch #%2d-   Batch #%5d:   %4.2f %% completed.' % (i + 1, batch_count, completion))\n",
    "                        a_t, c_t = self.test(train_data)\n",
    "                        a, c = self.test(validate_data)\n",
    "                        accuracy_hist_train.append(a)\n",
    "                        cost_hist_train.append(c)\n",
    "\n",
    "                        if sum(cost_hist_train[-4:]) > sum(cost_hist_train[-8:-4]):\n",
    "                            patience = patience - 1\n",
    "                        else:\n",
    "                            patience = 2\n",
    "\n",
    "                        if patience == 0:\n",
    "                            print(\"Cost Too High, Early Stop Activated\")\n",
    "                            save_path = saver.save(self.session, \"../models/\" + arg_2 + '_' + arg_3 + \".ckpt\")\n",
    "                            print(\"Model saved in path: %s\" % save_path)\n",
    "                            return\n",
    "                            \n",
    "        print(\"The training has been completed\")\n",
    "        save_path = saver.save(self.session, \"../models/\" + arg_2 + '_' + arg_3 + \".ckpt\")\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
